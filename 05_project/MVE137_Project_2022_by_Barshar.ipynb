{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42d4e64",
   "metadata": {},
   "source": [
    "**Project Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325c54a",
   "metadata": {},
   "source": [
    "**Formalities**\n",
    "\n",
    "This is the project for the course Probability and Statistical Learning Using Python, 2022. Here, you are asked to carry out the analysis using the tools, techniques, and skills acquired in the course and hand in a .pynb file with the solutions.\n",
    "\n",
    "The **deadline  is Friday, October\n",
    "28, 2022.** You should upload the solution file to 'Project Assignment' in Canvas via 'Home-->Project Assignment'.\n",
    "Note that this is an individual exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab58442",
   "metadata": {},
   "source": [
    "**Part I**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a735f",
   "metadata": {},
   "source": [
    "In this exercise we will estimate the test error of logistic regression model using the below described validation set approach. You will neeed to import the *Default.csv* file provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679f0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "import patsy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b68f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    default  student      balance       income\n",
      "0         0        0   729.526495  44361.62507\n",
      "1         0        1   817.180407  12106.13470\n",
      "2         0        0  1073.549164  31767.13895\n",
      "3         0        0   529.250605  35704.49394\n",
      "4         0        0   785.655883  38463.49588\n",
      "..      ...      ...          ...          ...\n",
      "95        0        0   820.017113  51584.65732\n",
      "96        0        1   619.751869  15750.62208\n",
      "97        0        0  1047.718124  46416.97099\n",
      "98        0        0   243.841328  47193.88813\n",
      "99        0        0   186.500387  45430.55027\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "df = pd.read_csv('Default.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df['default'] = df['default'].map({'No': 0, 'Yes': 1})\n",
    "df['student'] = df['student'].map({'No': 0, 'Yes': 1})\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6928055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.62507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.13470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.13895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.49394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.49588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>820.017113</td>\n",
       "      <td>51584.65732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>619.751869</td>\n",
       "      <td>15750.62208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1047.718124</td>\n",
       "      <td>46416.97099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243.841328</td>\n",
       "      <td>47193.88813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186.500387</td>\n",
       "      <td>45430.55027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    default  student      balance       income\n",
       "0         0        0   729.526495  44361.62507\n",
       "1         0        1   817.180407  12106.13470\n",
       "2         0        0  1073.549164  31767.13895\n",
       "3         0        0   529.250605  35704.49394\n",
       "4         0        0   785.655883  38463.49588\n",
       "..      ...      ...          ...          ...\n",
       "95        0        0   820.017113  51584.65732\n",
       "96        0        1   619.751869  15750.62208\n",
       "97        0        0  1047.718124  46416.97099\n",
       "98        0        0   243.841328  47193.88813\n",
       "99        0        0   186.500387  45430.55027\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e4c6d",
   "metadata": {},
   "source": [
    "(a) Fit a logistic regression model that uses $income$ and $balance$ to\n",
    "predict $default$ and print out the summary. **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786cea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n",
      "summary:                             Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Tue, 13 Dec 2022   Pseudo R-squ.:                  0.4594\n",
      "Time:                        15:21:52   Log-Likelihood:                -789.48\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "Accuracy:  0.966\n",
      "test error:  0.03400000000000003\n"
     ]
    }
   ],
   "source": [
    "X1 = df[['income', 'balance']]\n",
    "y1 = df['default']\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=5)\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X1_train, y1_train)\n",
    "y1_pred = logit.predict(X1_test)\n",
    "print(\"summary: \", smf.logit('default ~ income + balance', data=df).fit().summary())\n",
    "print('Accuracy: ', accuracy_score(y1_test, y1_pred))\n",
    "print(\"test error: \", 1 - accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302291b",
   "metadata": {},
   "source": [
    "(b) You are supposed to estimate the test error of this model using the validation set approach described below. In order to do this, you must perform the following steps: **(4 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31129a5",
   "metadata": {},
   "source": [
    "i. Split the sample set into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff91e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X1, y1, test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41deb6",
   "metadata": {},
   "source": [
    "ii. Fit a multiple logistic regression model using only the training\n",
    "observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52306566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit2 = LogisticRegression()\n",
    "logit2.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e13126",
   "metadata": {},
   "source": [
    "iii. Obtain a prediction of default status for each individual in\n",
    "the validation set (test set) by computing the posterior probability of\n",
    "$default$ for that individual, and classifying the individual to\n",
    "the $default$ category if the posterior probability is greater\n",
    "than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e4618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = logit2.predict(X2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8bfa3b",
   "metadata": {},
   "source": [
    "iv. Compute the validation set error, which is the fraction of\n",
    "the observations in the validation set (test set) that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a636670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n",
      "summary:                             Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Tue, 13 Dec 2022   Pseudo R-squ.:                  0.4594\n",
      "Time:                        15:21:53   Log-Likelihood:                -789.48\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "error:  0.03400000000000003\n",
      "Accuracy:  0.966\n"
     ]
    }
   ],
   "source": [
    "print(\"summary: \", smf.logit('default ~ income + balance', data=df).fit().summary())\n",
    "print(\"error: \", 1 - accuracy_score(y2_test, y2_pred))\n",
    "print('Accuracy: ', accuracy_score(y2_test, y2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f375ea",
   "metadata": {},
   "source": [
    "(c) Now consider a logistic regression model that predicts the probability of default using $income$, $balance$, a dummy variable for $student$ and print the summary. Estimate the test error for this model using the validation\n",
    "set approach. Comment on the results. Does the inclusion of a dummy variable for student lead to a reduction in the test error? **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e54900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9663333333333334\n",
      "test error:  0.03366666666666662\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078577\n",
      "         Iterations 10\n",
      "summary:                             Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9996\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 13 Dec 2022   Pseudo R-squ.:                  0.4619\n",
      "Time:                        15:21:53   Log-Likelihood:                -785.77\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                3.257e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -10.8690      0.492    -22.079      0.000     -11.834      -9.904\n",
      "income      3.033e-06    8.2e-06      0.370      0.712    -1.3e-05    1.91e-05\n",
      "balance        0.0057      0.000     24.737      0.000       0.005       0.006\n",
      "student       -0.6468      0.236     -2.738      0.006      -1.110      -0.184\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.15 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "X2 = df[['income']]\n",
    "y2 = df['default']\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X2, y2, test_size=0.3, random_state=5)\n",
    "logit3 = LogisticRegression()\n",
    "logit3.fit(X3_train, y3_train)\n",
    "y3_pred = logit3.predict(X3_test)\n",
    "print('Accuracy: ', accuracy_score(y3_test, y3_pred))\n",
    "print(\"test error: \", 1 - accuracy_score(y3_test, y3_pred))\n",
    "print(\"summary: \", smf.logit('default ~ income + balance + student', data=df).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f6b09",
   "metadata": {},
   "source": [
    "**Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb2b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f0854",
   "metadata": {},
   "source": [
    "In this exercise, you will demonstrate your understanding of the KNN classification algorithm and test it on a breast cancer dataset. The algorithm should be implemented in pure python, without using the sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ce805",
   "metadata": {},
   "source": [
    "**KNN algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048b3fc",
   "metadata": {},
   "source": [
    "(a)  Implement a function of your own to perform KNN classification **without using the default available libraries such as KNeighborsClassifier() in sklearn**. You will need to consider the Euclidean distances between the features and data to be predicted (test data) when selecting the k-nearest neighbors. The function should take 3 inputs as 1) data set to train the model 2) data to test 3) number of neighbours (k). If *k* is set to a value less than or equal to the total classification groups, the function should give a warning, and  warn() function defined in the 'warning' module will be useful for that.\n",
    "\n",
    "The function should output *classification_result*, where *classification_result* is the result of your classifier. Further, you should provide a suitable measure of the confidence on the classification. Justify your choice. \n",
    "\n",
    "Please note that only a few basic libraries/modules are imported and thus you are expected to import the needed others. \n",
    "\n",
    "\n",
    "\n",
    "**Hint:** You may fill and complete the function given below.                **(5 pts)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d468242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_algotithm(traindata, testdata, k=5):\n",
    "    distance = []\n",
    "    for i in range(len(traindata)):\n",
    "        distance.append(sqrt((traindata[i][0] - testdata[0]) ** 2 + (traindata[i][1] - testdata[1]) ** 2))\n",
    "    distance = np.array(distance)\n",
    "    index = np.argsort(distance)\n",
    "    index = index[:k]\n",
    "    classification = []\n",
    "    for i in index:\n",
    "        classification.append(traindata[i][2])\n",
    "    classification = np.array(classification)\n",
    "    classification_result = np.bincount(classification).argmax()\n",
    "    return classification_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b1202",
   "metadata": {},
   "source": [
    "#### Now let's test the implemented KNN algorithm on the given breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd0f63",
   "metadata": {},
   "source": [
    "This dataset contains records of breast cancer patients. Here we will use the features (columns) to predict the correct cancer class (last column) for the patients in the dataset as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27af2716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>unif_cell_size</th>\n",
       "      <th>unif_cell_shape</th>\n",
       "      <th>marg_adhesion</th>\n",
       "      <th>single_epith_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chrom</th>\n",
       "      <th>norm_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1164066</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1165297</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1165790</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1165926</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1166630</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  clump_thickness  unif_cell_size  unif_cell_shape  marg_adhesion  \\\n",
       "0   1000025                5               1                1              1   \n",
       "1   1002945                5               4                4              5   \n",
       "2   1015425                3               1                1              1   \n",
       "3   1016277                6               8                8              1   \n",
       "4   1017023                4               1                1              3   \n",
       "..      ...              ...             ...              ...            ...   \n",
       "95  1164066                1               1                1              1   \n",
       "96  1165297                2               1                1              2   \n",
       "97  1165790                5               1                1              1   \n",
       "98  1165926                9               6                9              2   \n",
       "99  1166630                7               5                6             10   \n",
       "\n",
       "    single_epith_cell_size bare_nuclei  bland_chrom  norm_nucleoli  mitoses  \\\n",
       "0                        2           1            3              1        1   \n",
       "1                        7          10            3              2        1   \n",
       "2                        2           2            3              1        1   \n",
       "3                        3           4            3              7        1   \n",
       "4                        2           1            3              1        1   \n",
       "..                     ...         ...          ...            ...      ...   \n",
       "95                       2           1            3              1        1   \n",
       "96                       2           1            1              1        1   \n",
       "97                       2           1            3              1        1   \n",
       "98                      10           6            2              9       10   \n",
       "99                       5          10            7              9        4   \n",
       "\n",
       "    class  \n",
       "0       2  \n",
       "1       2  \n",
       "2       2  \n",
       "3       2  \n",
       "4       2  \n",
       "..    ...  \n",
       "95      2  \n",
       "96      2  \n",
       "97      2  \n",
       "98      4  \n",
       "99      4  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['id', 'clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion',\n",
    "           'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses', 'class']\n",
    "#Import the data file by uncommenting below and setting the path to the dataset\n",
    "data = pd.read_csv('breast-cancer-wisconsin.data', header=None, names=columns)\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb999d6b",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be7ad9",
   "metadata": {},
   "source": [
    "(b) Check how many different cancer classes are available, and plot a pie chart to see the distribution of classes. Then, find and replace all the missing values with the mode of the particular column(s). Note that the missing values are marked with '?' in the dataset. **(1 pt)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4883d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.replace('?', np.nan)\n",
    "data['class'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80206cd",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a7b6c",
   "metadata": {},
   "source": [
    "(c) Drop the obiviously unwanted column(s). **(1 pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67debe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis=1)\n",
    "data = data.dropna()\n",
    "data = data.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f60fc",
   "metadata": {},
   "source": [
    "#### Split the training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7067dc",
   "metadata": {},
   "source": [
    "(d) In here, you will split the data into training and test sets, **without using sklearn library.** Please use 20% of the dataset as test set and the rest for train set. Shuffle the data prior to splitting in order to prevent any bias during the training and to avoid the model from learning the order of the training. **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e09a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "train = data[:int(len(data) * 0.8)]\n",
    "test = data[int(len(data) * 0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eda4a2",
   "metadata": {},
   "source": [
    "#### Predict the class for test data and calculate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173dff6",
   "metadata": {},
   "source": [
    "(e) Now train the KNN classifier you developed in (a) using the training set, and test it on the test set with *k_neighbours=5*. **(4 pts)**\n",
    "\n",
    "Print the confidence for the incorrect predictions the classifier has made.\n",
    "\n",
    "Find the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bad73b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence for incorrect predictions:\n",
      "Confidence for prediction 0 : 0.6204379562043796\n",
      "Confidence for prediction 1 : 0.6204379562043796\n",
      "Confidence for prediction 2 : 0.051094890510948905\n",
      "Confidence for prediction 3 : 0.058394160583941604\n",
      "Confidence for prediction 4 : 0.6204379562043796\n",
      "Confidence for prediction 5 : 0.6204379562043796\n",
      "Confidence for prediction 7 : 0.6204379562043796\n",
      "Confidence for prediction 8 : 0.051094890510948905\n",
      "Confidence for prediction 9 : 0.6204379562043796\n",
      "Confidence for prediction 10 : 0.6204379562043796\n",
      "Confidence for prediction 12 : 0.6204379562043796\n",
      "Confidence for prediction 14 : 0.6204379562043796\n",
      "Confidence for prediction 15 : 0.6204379562043796\n",
      "Confidence for prediction 16 : 0.6204379562043796\n",
      "Confidence for prediction 17 : 0.6204379562043796\n",
      "Confidence for prediction 18 : 0.6204379562043796\n",
      "Confidence for prediction 19 : 0.058394160583941604\n",
      "Confidence for prediction 20 : 0.6204379562043796\n",
      "Confidence for prediction 21 : 0.6204379562043796\n",
      "Confidence for prediction 22 : 0.058394160583941604\n",
      "Confidence for prediction 24 : 0.0364963503649635\n",
      "Confidence for prediction 25 : 0.6204379562043796\n",
      "Confidence for prediction 26 : 0.6204379562043796\n",
      "Confidence for prediction 27 : 0.6204379562043796\n",
      "Confidence for prediction 28 : 0.6204379562043796\n",
      "Confidence for prediction 29 : 0.6204379562043796\n",
      "Confidence for prediction 30 : 0.014598540145985401\n",
      "Confidence for prediction 32 : 0.072992700729927\n",
      "Confidence for prediction 33 : 0.6204379562043796\n",
      "Confidence for prediction 34 : 0.6204379562043796\n",
      "Confidence for prediction 36 : 0.6204379562043796\n",
      "Confidence for prediction 37 : 0.058394160583941604\n",
      "Confidence for prediction 38 : 0.6204379562043796\n",
      "Confidence for prediction 39 : 0.6204379562043796\n",
      "Confidence for prediction 40 : 0.6204379562043796\n",
      "Confidence for prediction 41 : 0.6204379562043796\n",
      "Confidence for prediction 42 : 0.6204379562043796\n",
      "Confidence for prediction 45 : 0.6204379562043796\n",
      "Confidence for prediction 46 : 0.6204379562043796\n",
      "Confidence for prediction 47 : 0.6204379562043796\n",
      "Confidence for prediction 49 : 0.6204379562043796\n",
      "Confidence for prediction 50 : 0.072992700729927\n",
      "Confidence for prediction 51 : 0.6204379562043796\n",
      "Confidence for prediction 52 : 0.6204379562043796\n",
      "Confidence for prediction 53 : 0.6204379562043796\n",
      "Confidence for prediction 54 : 0.058394160583941604\n",
      "Confidence for prediction 55 : 0.6204379562043796\n",
      "Confidence for prediction 56 : 0.6204379562043796\n",
      "Confidence for prediction 57 : 0.021897810218978103\n",
      "Confidence for prediction 58 : 0.6204379562043796\n",
      "Confidence for prediction 59 : 0.051094890510948905\n",
      "Confidence for prediction 60 : 0.6204379562043796\n",
      "Confidence for prediction 61 : 0.6204379562043796\n",
      "Confidence for prediction 62 : 0.0364963503649635\n",
      "Confidence for prediction 63 : 0.6204379562043796\n",
      "Confidence for prediction 64 : 0.6204379562043796\n",
      "Confidence for prediction 65 : 0.6204379562043796\n",
      "Confidence for prediction 66 : 0.051094890510948905\n",
      "Confidence for prediction 67 : 0.6204379562043796\n",
      "Confidence for prediction 68 : 0.0364963503649635\n",
      "Confidence for prediction 69 : 0.6204379562043796\n",
      "Confidence for prediction 70 : 0.072992700729927\n",
      "Confidence for prediction 71 : 0.0364963503649635\n",
      "Confidence for prediction 72 : 0.6204379562043796\n",
      "Confidence for prediction 73 : 0.6204379562043796\n",
      "Confidence for prediction 75 : 0.6204379562043796\n",
      "Confidence for prediction 76 : 0.6204379562043796\n",
      "Confidence for prediction 77 : 0.6204379562043796\n",
      "Confidence for prediction 78 : 0.6204379562043796\n",
      "Confidence for prediction 79 : 0.072992700729927\n",
      "Confidence for prediction 80 : 0.6204379562043796\n",
      "Confidence for prediction 81 : 0.6204379562043796\n",
      "Confidence for prediction 82 : 0.6204379562043796\n",
      "Confidence for prediction 83 : 0.051094890510948905\n",
      "Confidence for prediction 85 : 0.6204379562043796\n",
      "Confidence for prediction 86 : 0.6204379562043796\n",
      "Confidence for prediction 87 : 0.6204379562043796\n",
      "Confidence for prediction 88 : 0.6204379562043796\n",
      "Confidence for prediction 89 : 0.072992700729927\n",
      "Confidence for prediction 90 : 0.6204379562043796\n",
      "Confidence for prediction 91 : 0.6204379562043796\n",
      "Confidence for prediction 92 : 0.051094890510948905\n",
      "Confidence for prediction 93 : 0.058394160583941604\n",
      "Confidence for prediction 94 : 0.6204379562043796\n",
      "Confidence for prediction 95 : 0.6204379562043796\n",
      "Confidence for prediction 96 : 0.6204379562043796\n",
      "Confidence for prediction 97 : 0.021897810218978103\n",
      "Confidence for prediction 98 : 0.6204379562043796\n",
      "Confidence for prediction 99 : 0.6204379562043796\n",
      "Confidence for prediction 100 : 0.6204379562043796\n",
      "Confidence for prediction 101 : 0.6204379562043796\n",
      "Confidence for prediction 102 : 0.6204379562043796\n",
      "Confidence for prediction 103 : 0.072992700729927\n",
      "Confidence for prediction 105 : 0.0364963503649635\n",
      "Confidence for prediction 106 : 0.6204379562043796\n",
      "Confidence for prediction 107 : 0.6204379562043796\n",
      "Confidence for prediction 108 : 0.051094890510948905\n",
      "Confidence for prediction 109 : 0.072992700729927\n",
      "Confidence for prediction 110 : 0.6204379562043796\n",
      "Confidence for prediction 111 : 0.6204379562043796\n",
      "Confidence for prediction 112 : 0.6204379562043796\n",
      "Confidence for prediction 113 : 0.6204379562043796\n",
      "Confidence for prediction 114 : 0.058394160583941604\n",
      "Confidence for prediction 115 : 0.6204379562043796\n",
      "Confidence for prediction 116 : 0.072992700729927\n",
      "Confidence for prediction 117 : 0.6204379562043796\n",
      "Confidence for prediction 118 : 0.6204379562043796\n",
      "Confidence for prediction 119 : 0.6204379562043796\n",
      "Confidence for prediction 121 : 0.021897810218978103\n",
      "Confidence for prediction 123 : 0.6204379562043796\n",
      "Confidence for prediction 124 : 0.014598540145985401\n",
      "Confidence for prediction 125 : 0.058394160583941604\n",
      "Confidence for prediction 127 : 0.6204379562043796\n",
      "Confidence for prediction 129 : 0.072992700729927\n",
      "Confidence for prediction 130 : 0.6204379562043796\n",
      "Confidence for prediction 131 : 0.6204379562043796\n",
      "Confidence for prediction 132 : 0.6204379562043796\n",
      "Confidence for prediction 133 : 0.6204379562043796\n",
      "Confidence for prediction 134 : 0.072992700729927\n",
      "Confidence for prediction 135 : 0.6204379562043796\n",
      "Confidence for prediction 136 : 0.072992700729927\n",
      "Accuracy: 0.8832116788321168\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "traindata = train.values\n",
    "testdata = test.values\n",
    "k = 5\n",
    "correct = 0\n",
    "result = []\n",
    "for i in range(len(testdata)):\n",
    "    result.append(knn_algotithm(traindata, testdata[i], k))\n",
    "result=np.array(result)\n",
    "test_result = test['class'].values\n",
    "print('Confidence for incorrect predictions:')\n",
    "for i in range(len(result)):\n",
    "    if result[i] != test_result[i]:\n",
    "        print('Confidence for prediction', i, ':', np.bincount(result)[result[i]] / len(result))\n",
    "        correct += 1\n",
    "print('Accuracy:', correct / len(result))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f358925",
   "metadata": {},
   "source": [
    "**Effect of reduction in training data size on confidence of predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5137829",
   "metadata": {},
   "source": [
    "(f) Now increase the test set size to 40% of the dataset while keeping the same *k_neighbours* and print the confidence and accuracy of the predictions similar to the previous question. Explain the results in comparison with (e) **(2 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad9f89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence for incorrect predictions:\n",
      "Confidence for prediction 0 : 0.583941605839416\n",
      "Confidence for prediction 1 : 0.08394160583941605\n",
      "Confidence for prediction 2 : 0.583941605839416\n",
      "Confidence for prediction 3 : 0.08394160583941605\n",
      "Confidence for prediction 4 : 0.583941605839416\n",
      "Confidence for prediction 5 : 0.583941605839416\n",
      "Confidence for prediction 6 : 0.583941605839416\n",
      "Confidence for prediction 7 : 0.01824817518248175\n",
      "Confidence for prediction 9 : 0.583941605839416\n",
      "Confidence for prediction 10 : 0.583941605839416\n",
      "Confidence for prediction 12 : 0.583941605839416\n",
      "Confidence for prediction 13 : 0.08394160583941605\n",
      "Confidence for prediction 14 : 0.08394160583941605\n",
      "Confidence for prediction 15 : 0.583941605839416\n",
      "Confidence for prediction 16 : 0.08394160583941605\n",
      "Confidence for prediction 17 : 0.058394160583941604\n",
      "Confidence for prediction 18 : 0.583941605839416\n",
      "Confidence for prediction 19 : 0.583941605839416\n",
      "Confidence for prediction 21 : 0.051094890510948905\n",
      "Confidence for prediction 22 : 0.583941605839416\n",
      "Confidence for prediction 23 : 0.583941605839416\n",
      "Confidence for prediction 24 : 0.12773722627737227\n",
      "Confidence for prediction 25 : 0.583941605839416\n",
      "Confidence for prediction 26 : 0.583941605839416\n",
      "Confidence for prediction 27 : 0.583941605839416\n",
      "Confidence for prediction 28 : 0.583941605839416\n",
      "Confidence for prediction 29 : 0.583941605839416\n",
      "Confidence for prediction 30 : 0.583941605839416\n",
      "Confidence for prediction 31 : 0.583941605839416\n",
      "Confidence for prediction 32 : 0.583941605839416\n",
      "Confidence for prediction 34 : 0.583941605839416\n",
      "Confidence for prediction 35 : 0.583941605839416\n",
      "Confidence for prediction 36 : 0.051094890510948905\n",
      "Confidence for prediction 37 : 0.583941605839416\n",
      "Confidence for prediction 38 : 0.051094890510948905\n",
      "Confidence for prediction 39 : 0.08394160583941605\n",
      "Confidence for prediction 40 : 0.583941605839416\n",
      "Confidence for prediction 41 : 0.058394160583941604\n",
      "Confidence for prediction 42 : 0.051094890510948905\n",
      "Confidence for prediction 43 : 0.12773722627737227\n",
      "Confidence for prediction 44 : 0.12773722627737227\n",
      "Confidence for prediction 45 : 0.583941605839416\n",
      "Confidence for prediction 46 : 0.583941605839416\n",
      "Confidence for prediction 47 : 0.583941605839416\n",
      "Confidence for prediction 48 : 0.12773722627737227\n",
      "Confidence for prediction 49 : 0.583941605839416\n",
      "Confidence for prediction 50 : 0.12773722627737227\n",
      "Confidence for prediction 51 : 0.583941605839416\n",
      "Confidence for prediction 52 : 0.583941605839416\n",
      "Confidence for prediction 53 : 0.583941605839416\n",
      "Confidence for prediction 54 : 0.583941605839416\n",
      "Confidence for prediction 55 : 0.058394160583941604\n",
      "Confidence for prediction 56 : 0.12773722627737227\n",
      "Confidence for prediction 57 : 0.583941605839416\n",
      "Confidence for prediction 58 : 0.08394160583941605\n",
      "Confidence for prediction 59 : 0.583941605839416\n",
      "Confidence for prediction 60 : 0.583941605839416\n",
      "Confidence for prediction 61 : 0.583941605839416\n",
      "Confidence for prediction 62 : 0.01824817518248175\n",
      "Confidence for prediction 63 : 0.583941605839416\n",
      "Confidence for prediction 64 : 0.08394160583941605\n",
      "Confidence for prediction 65 : 0.583941605839416\n",
      "Confidence for prediction 67 : 0.051094890510948905\n",
      "Confidence for prediction 68 : 0.08394160583941605\n",
      "Confidence for prediction 69 : 0.583941605839416\n",
      "Confidence for prediction 70 : 0.583941605839416\n",
      "Confidence for prediction 71 : 0.583941605839416\n",
      "Confidence for prediction 72 : 0.583941605839416\n",
      "Confidence for prediction 73 : 0.583941605839416\n",
      "Confidence for prediction 74 : 0.583941605839416\n",
      "Confidence for prediction 76 : 0.12773722627737227\n",
      "Confidence for prediction 77 : 0.583941605839416\n",
      "Confidence for prediction 78 : 0.12773722627737227\n",
      "Confidence for prediction 79 : 0.08394160583941605\n",
      "Confidence for prediction 80 : 0.583941605839416\n",
      "Confidence for prediction 81 : 0.058394160583941604\n",
      "Confidence for prediction 82 : 0.08394160583941605\n",
      "Confidence for prediction 83 : 0.583941605839416\n",
      "Confidence for prediction 84 : 0.583941605839416\n",
      "Confidence for prediction 85 : 0.12773722627737227\n",
      "Confidence for prediction 86 : 0.583941605839416\n",
      "Confidence for prediction 87 : 0.583941605839416\n",
      "Confidence for prediction 88 : 0.08394160583941605\n",
      "Confidence for prediction 89 : 0.583941605839416\n",
      "Confidence for prediction 90 : 0.058394160583941604\n",
      "Confidence for prediction 91 : 0.051094890510948905\n",
      "Confidence for prediction 92 : 0.01824817518248175\n",
      "Confidence for prediction 93 : 0.583941605839416\n",
      "Confidence for prediction 94 : 0.058394160583941604\n",
      "Confidence for prediction 95 : 0.583941605839416\n",
      "Confidence for prediction 96 : 0.583941605839416\n",
      "Confidence for prediction 97 : 0.12773722627737227\n",
      "Confidence for prediction 98 : 0.583941605839416\n",
      "Confidence for prediction 99 : 0.583941605839416\n",
      "Confidence for prediction 100 : 0.583941605839416\n",
      "Confidence for prediction 101 : 0.12773722627737227\n",
      "Confidence for prediction 103 : 0.583941605839416\n",
      "Confidence for prediction 104 : 0.583941605839416\n",
      "Confidence for prediction 105 : 0.058394160583941604\n",
      "Confidence for prediction 106 : 0.583941605839416\n",
      "Confidence for prediction 107 : 0.051094890510948905\n",
      "Confidence for prediction 109 : 0.583941605839416\n",
      "Confidence for prediction 110 : 0.583941605839416\n",
      "Confidence for prediction 111 : 0.12773722627737227\n",
      "Confidence for prediction 112 : 0.583941605839416\n",
      "Confidence for prediction 113 : 0.583941605839416\n",
      "Confidence for prediction 115 : 0.583941605839416\n",
      "Confidence for prediction 116 : 0.583941605839416\n",
      "Confidence for prediction 117 : 0.583941605839416\n",
      "Confidence for prediction 118 : 0.08394160583941605\n",
      "Confidence for prediction 119 : 0.583941605839416\n",
      "Confidence for prediction 120 : 0.583941605839416\n",
      "Confidence for prediction 121 : 0.583941605839416\n",
      "Confidence for prediction 122 : 0.583941605839416\n",
      "Confidence for prediction 123 : 0.583941605839416\n",
      "Confidence for prediction 124 : 0.583941605839416\n",
      "Confidence for prediction 125 : 0.08394160583941605\n",
      "Confidence for prediction 126 : 0.583941605839416\n",
      "Confidence for prediction 127 : 0.583941605839416\n",
      "Confidence for prediction 128 : 0.12773722627737227\n",
      "Confidence for prediction 129 : 0.583941605839416\n",
      "Confidence for prediction 130 : 0.583941605839416\n",
      "Confidence for prediction 131 : 0.583941605839416\n",
      "Confidence for prediction 132 : 0.058394160583941604\n",
      "Confidence for prediction 133 : 0.12773722627737227\n",
      "Confidence for prediction 134 : 0.12773722627737227\n",
      "Confidence for prediction 135 : 0.583941605839416\n",
      "Confidence for prediction 136 : 0.583941605839416\n",
      "Confidence for prediction 137 : 0.583941605839416\n",
      "Confidence for prediction 138 : 0.583941605839416\n",
      "Confidence for prediction 139 : 0.051094890510948905\n",
      "Confidence for prediction 140 : 0.12773722627737227\n",
      "Confidence for prediction 141 : 0.583941605839416\n",
      "Confidence for prediction 142 : 0.583941605839416\n",
      "Confidence for prediction 143 : 0.12773722627737227\n",
      "Confidence for prediction 144 : 0.051094890510948905\n",
      "Confidence for prediction 145 : 0.12773722627737227\n",
      "Confidence for prediction 146 : 0.08394160583941605\n",
      "Confidence for prediction 147 : 0.583941605839416\n",
      "Confidence for prediction 148 : 0.583941605839416\n",
      "Confidence for prediction 149 : 0.583941605839416\n",
      "Confidence for prediction 150 : 0.583941605839416\n",
      "Confidence for prediction 151 : 0.583941605839416\n",
      "Confidence for prediction 152 : 0.583941605839416\n",
      "Confidence for prediction 153 : 0.583941605839416\n",
      "Confidence for prediction 154 : 0.12773722627737227\n",
      "Confidence for prediction 155 : 0.051094890510948905\n",
      "Confidence for prediction 156 : 0.058394160583941604\n",
      "Confidence for prediction 157 : 0.583941605839416\n",
      "Confidence for prediction 158 : 0.583941605839416\n",
      "Confidence for prediction 159 : 0.583941605839416\n",
      "Confidence for prediction 160 : 0.583941605839416\n",
      "Confidence for prediction 161 : 0.583941605839416\n",
      "Confidence for prediction 162 : 0.08394160583941605\n",
      "Confidence for prediction 163 : 0.01824817518248175\n",
      "Confidence for prediction 165 : 0.583941605839416\n",
      "Confidence for prediction 166 : 0.583941605839416\n",
      "Confidence for prediction 167 : 0.583941605839416\n",
      "Confidence for prediction 168 : 0.583941605839416\n",
      "Confidence for prediction 169 : 0.058394160583941604\n",
      "Confidence for prediction 170 : 0.051094890510948905\n",
      "Confidence for prediction 171 : 0.583941605839416\n",
      "Confidence for prediction 172 : 0.583941605839416\n",
      "Confidence for prediction 173 : 0.583941605839416\n",
      "Confidence for prediction 174 : 0.08394160583941605\n",
      "Confidence for prediction 175 : 0.12773722627737227\n",
      "Confidence for prediction 176 : 0.583941605839416\n",
      "Confidence for prediction 177 : 0.583941605839416\n",
      "Confidence for prediction 178 : 0.583941605839416\n",
      "Confidence for prediction 179 : 0.12773722627737227\n",
      "Confidence for prediction 180 : 0.583941605839416\n",
      "Confidence for prediction 181 : 0.01824817518248175\n",
      "Confidence for prediction 182 : 0.583941605839416\n",
      "Confidence for prediction 183 : 0.12773722627737227\n",
      "Confidence for prediction 184 : 0.0072992700729927005\n",
      "Confidence for prediction 185 : 0.08394160583941605\n",
      "Confidence for prediction 186 : 0.583941605839416\n",
      "Confidence for prediction 187 : 0.583941605839416\n",
      "Confidence for prediction 188 : 0.051094890510948905\n",
      "Confidence for prediction 189 : 0.583941605839416\n",
      "Confidence for prediction 192 : 0.583941605839416\n",
      "Confidence for prediction 193 : 0.583941605839416\n",
      "Confidence for prediction 195 : 0.583941605839416\n",
      "Confidence for prediction 196 : 0.583941605839416\n",
      "Confidence for prediction 197 : 0.583941605839416\n",
      "Confidence for prediction 198 : 0.08394160583941605\n",
      "Confidence for prediction 199 : 0.058394160583941604\n",
      "Confidence for prediction 200 : 0.12773722627737227\n",
      "Confidence for prediction 201 : 0.12773722627737227\n",
      "Confidence for prediction 203 : 0.08394160583941605\n",
      "Confidence for prediction 204 : 0.583941605839416\n",
      "Confidence for prediction 205 : 0.0072992700729927005\n",
      "Confidence for prediction 206 : 0.058394160583941604\n",
      "Confidence for prediction 207 : 0.583941605839416\n",
      "Confidence for prediction 208 : 0.583941605839416\n",
      "Confidence for prediction 210 : 0.08394160583941605\n",
      "Confidence for prediction 211 : 0.12773722627737227\n",
      "Confidence for prediction 212 : 0.583941605839416\n",
      "Confidence for prediction 213 : 0.583941605839416\n",
      "Confidence for prediction 214 : 0.583941605839416\n",
      "Confidence for prediction 215 : 0.583941605839416\n",
      "Confidence for prediction 216 : 0.583941605839416\n",
      "Confidence for prediction 217 : 0.583941605839416\n",
      "Confidence for prediction 218 : 0.583941605839416\n",
      "Confidence for prediction 219 : 0.583941605839416\n",
      "Confidence for prediction 220 : 0.583941605839416\n",
      "Confidence for prediction 221 : 0.058394160583941604\n",
      "Confidence for prediction 222 : 0.583941605839416\n",
      "Confidence for prediction 223 : 0.583941605839416\n",
      "Confidence for prediction 224 : 0.08394160583941605\n",
      "Confidence for prediction 225 : 0.12773722627737227\n",
      "Confidence for prediction 226 : 0.583941605839416\n",
      "Confidence for prediction 227 : 0.12773722627737227\n",
      "Confidence for prediction 228 : 0.583941605839416\n",
      "Confidence for prediction 229 : 0.583941605839416\n",
      "Confidence for prediction 230 : 0.12773722627737227\n",
      "Confidence for prediction 231 : 0.12773722627737227\n",
      "Confidence for prediction 232 : 0.12773722627737227\n",
      "Confidence for prediction 233 : 0.043795620437956206\n",
      "Confidence for prediction 234 : 0.12773722627737227\n",
      "Confidence for prediction 235 : 0.025547445255474453\n",
      "Confidence for prediction 236 : 0.583941605839416\n",
      "Confidence for prediction 237 : 0.058394160583941604\n",
      "Confidence for prediction 238 : 0.051094890510948905\n",
      "Confidence for prediction 239 : 0.583941605839416\n",
      "Confidence for prediction 240 : 0.583941605839416\n",
      "Confidence for prediction 241 : 0.08394160583941605\n",
      "Confidence for prediction 242 : 0.583941605839416\n",
      "Confidence for prediction 243 : 0.583941605839416\n",
      "Confidence for prediction 244 : 0.058394160583941604\n",
      "Confidence for prediction 245 : 0.583941605839416\n",
      "Confidence for prediction 246 : 0.583941605839416\n",
      "Confidence for prediction 247 : 0.583941605839416\n",
      "Confidence for prediction 248 : 0.583941605839416\n",
      "Confidence for prediction 249 : 0.583941605839416\n",
      "Confidence for prediction 250 : 0.051094890510948905\n",
      "Confidence for prediction 251 : 0.583941605839416\n",
      "Confidence for prediction 252 : 0.058394160583941604\n",
      "Confidence for prediction 253 : 0.583941605839416\n",
      "Confidence for prediction 254 : 0.583941605839416\n",
      "Confidence for prediction 255 : 0.583941605839416\n",
      "Confidence for prediction 256 : 0.583941605839416\n",
      "Confidence for prediction 257 : 0.583941605839416\n",
      "Confidence for prediction 258 : 0.12773722627737227\n",
      "Confidence for prediction 259 : 0.583941605839416\n",
      "Confidence for prediction 261 : 0.583941605839416\n",
      "Confidence for prediction 262 : 0.043795620437956206\n",
      "Confidence for prediction 263 : 0.583941605839416\n",
      "Confidence for prediction 264 : 0.583941605839416\n",
      "Confidence for prediction 265 : 0.583941605839416\n",
      "Confidence for prediction 266 : 0.583941605839416\n",
      "Confidence for prediction 267 : 0.583941605839416\n",
      "Confidence for prediction 268 : 0.12773722627737227\n",
      "Confidence for prediction 269 : 0.583941605839416\n",
      "Confidence for prediction 270 : 0.12773722627737227\n",
      "Confidence for prediction 271 : 0.583941605839416\n",
      "Confidence for prediction 272 : 0.12773722627737227\n",
      "Confidence for prediction 273 : 0.583941605839416\n",
      "Accuracy: 0.9416058394160584\n"
     ]
    }
   ],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "train = data[:int(len(data) * 0.6)]\n",
    "test = data[int(len(data) * 0.6):]\n",
    "traindata = train.values\n",
    "testdata = test.values\n",
    "k = 5\n",
    "correct = 0\n",
    "result = []\n",
    "for i in range(len(testdata)):\n",
    "    result.append(knn_algotithm(traindata, testdata[i], k))\n",
    "result=np.array(result)\n",
    "test_result = test['class'].values\n",
    "print('Confidence for incorrect predictions:')\n",
    "for i in range(len(result)):\n",
    "    if result[i] != test_result[i]:\n",
    "        print('Confidence for prediction', i, ':', np.bincount(result)[result[i]] / len(result))\n",
    "        correct += 1\n",
    "print('Accuracy:', correct / len(result))\n",
    "\n",
    "# The accuracy is lower than the previous case. This is because the test set is larger, so the model is less likely to be able to predict the correct class.\n",
    "# The confidence for incorrect predictions is higher than the previous case. This is because the test set is larger, so the model is less likely to be able to predict the correct class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a84cc8",
   "metadata": {},
   "source": [
    "#### Alternative classification methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c8ec6",
   "metadata": {},
   "source": [
    "(g) Propose alternative classification approaches for this problem and discuss the advantages and disadvantages with respect to KNNs (you don't need to implement them). **(2 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3168fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The advantages of KNN are that it is simple and easy to implement. The disadvantages are that it is computationally expensive and it is not suitable for large datasets.\n",
    "\n",
    "# The advantages of Decision Tree are that it is simple and easy to implement. The disadvantages are that it is not suitable for large datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
